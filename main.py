#!/usr/bin/env python3
"""
ORQUESTADOR ETL BATCH - SISTEMA PRINCIPAL
=========================================
Orquestador principal del sistema ETL con procesamiento por lotes
optimizado para Ubuntu 22.04
"""

import sys
import os
from pathlib import Path
import logging
from datetime import datetime
import yaml
import click
from dotenv import load_dotenv
from typing import Dict, Any, List
import psycopg2
from psycopg2.extras import execute_values

# Agregar ruta del proyecto
sys.path.insert(0, str(Path(__file__).parent))

from core.batch_processor import BatchProcessor, BatchConfig, StreamingBatchProcessor
from core.data_validator import DataValidator
from extractors.database_extractor import DatabaseExtractor
from extractors.csv_extractor import CSVExtractor
from transformers.complete_dimension_builder import CompleteDimensionBuilder
from transformers.complete_fact_builder import CompleteFactBuilder
from loaders.database_loader import DatabaseLoader
from utils.logger import setup_logger
from utils.metrics import MetricsCollector


class ETLOrchestrator:
    """Orquestador principal del ETL"""

    def __init__(self, config_path: Path = None):
        """
        Inicializa el orquestador

        Args:
            config_path: Ruta al archivo de configuraci√≥n
        """
        # Cargar configuraci√≥n
        if config_path is None:
            config_path = Path(__file__).parent / "config" / "etl_config.yaml"

        with open(config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)

        # Cargar variables de entorno
        env_file = Path(__file__).parent / ".env"
        if env_file.exists():
            load_dotenv(env_file)

        # Configurar logger
        self.logger = setup_logger("ETLOrchestrator", self.config["paths"]["logs"])

        # Inicializar componentes
        batch_config = BatchConfig(
            chunk_size=self.config["batch"]["chunk_size"],
            max_workers=self.config["batch"]["max_workers"],
            timeout=self.config["batch"]["timeout"],
            max_retries=self.config["batch"]["max_retries"],
            retry_delay=self.config["batch"]["retry_delay"],
            max_memory_mb=self.config["batch"]["max_memory_mb"],
            enable_checkpoints=self.config["recovery"]["enable_checkpoints"],
            checkpoint_interval=self.config["recovery"]["checkpoint_interval"],
        )

        self.batch_processor = BatchProcessor(
            batch_config, Path(self.config["paths"]["checkpoints"])
        )

        self.streaming_processor = StreamingBatchProcessor(
            batch_config, Path(self.config["paths"]["checkpoints"])
        )

        self.data_validator = DataValidator(self.config)

        self.db_extractor = DatabaseExtractor(self.config)
        self.csv_extractor = CSVExtractor(self.config)

        self.dimension_builder = CompleteDimensionBuilder()
        self.fact_builder = CompleteFactBuilder()

        self.db_loader = DatabaseLoader(self.config)

        self.metrics = MetricsCollector()

        self.logger.info("üöÄ Orquestador ETL inicializado")

    def run_full_etl(self) -> Dict[str, Any]:
        """
        Ejecuta el proceso ETL completo

        Returns:
            Diccionario con resultados de la ejecuci√≥n
        """
        self.logger.info("=" * 80)
        self.logger.info("üè™ PUNTAFINA ETL BATCH - PROCESO COMPLETO")
        self.logger.info("=" * 80)

        start_time = datetime.now()

        try:
            # -1. Desbloquear tablas forzadamente
            self.logger.info("\nüîì FASE -1: DESBLOQUEO FORZADO DE TABLAS")
            self._force_unlock_tables()

            # 1. Extracci√≥n
            self.logger.info("\nüì• FASE 1: EXTRACCI√ìN")
            extraction_results = self._run_extraction()

            # 2. Transformaci√≥n - Dimensiones
            self.logger.info("\nüîÑ FASE 2: TRANSFORMACI√ìN - DIMENSIONES")
            dimension_results = self._run_dimension_building()

            # 3. Transformaci√≥n - Facts
            self.logger.info("\nüîÑ FASE 3: TRANSFORMACI√ìN - TABLAS DE HECHOS")
            fact_results = self._run_fact_building()

            # 4. Carga
            self.logger.info("\nüì§ FASE 4: CARGA")
            loading_results = self._run_loading()

            # 5. Validaci√≥n final
            self.logger.info("\n‚úÖ FASE 5: VALIDACI√ìN FINAL")
            validation_results = self._run_final_validation()

            elapsed_time = (datetime.now() - start_time).total_seconds()

            # Reporte final
            final_report = {
                "status": "success",
                "start_time": start_time.isoformat(),
                "end_time": datetime.now().isoformat(),
                "elapsed_time": elapsed_time,
                "extraction": extraction_results,
                "dimensions": dimension_results,
                "facts": fact_results,
                "loading": loading_results,
                "validation": validation_results,
                "metrics": self.metrics.get_summary(),
            }

            self._print_final_summary(final_report)

            return final_report

        except Exception as e:
            self.logger.error(f"‚ùå Error en proceso ETL: {e}", exc_info=True)
            raise

    def _force_unlock_tables(self):
        """Desbloquear forzadamente todas las tablas eliminando conexiones idle y locks"""
        import psycopg2

        try:
            conn = psycopg2.connect(
                host=os.getenv("DW_DB_HOST"),
                port=int(os.getenv("DW_DB_PORT")),
                dbname=os.getenv("DW_DB_NAME"),
                user=os.getenv("DW_DB_USER"),
                password=os.getenv("DW_DB_PASS"),
                connect_timeout=30,
            )
            cursor = conn.cursor()

            # 1. Terminar todas las conexiones idle in transaction
            self.logger.info("   üí• Terminando conexiones idle...")
            cursor.execute(
                """
                SELECT pg_terminate_backend(pid), pid, usename, state, query_start
                FROM pg_stat_activity 
                WHERE datname = current_database() 
                AND pid <> pg_backend_pid()
                AND state IN ('idle in transaction', 'idle in transaction (aborted)')
            """
            )
            terminated = cursor.fetchall()
            if terminated:
                self.logger.info(f"   ‚úì Terminadas {len(terminated)} conexiones idle")

            # 2. Cancelar queries largas (m√°s de 5 minutos)
            self.logger.info("   ‚è±Ô∏è  Cancelando queries largas...")
            cursor.execute(
                """
                SELECT pg_cancel_backend(pid), pid, usename, 
                       EXTRACT(EPOCH FROM (NOW() - query_start)) as duration
                FROM pg_stat_activity 
                WHERE datname = current_database() 
                AND pid <> pg_backend_pid()
                AND state = 'active'
                AND query_start < NOW() - INTERVAL '5 minutes'
                AND query NOT LIKE '%pg_stat_activity%'
            """
            )
            cancelled = cursor.fetchall()
            if cancelled:
                self.logger.info(f"   ‚úì Canceladas {len(cancelled)} queries largas")

            # 3. Liberar locks de tablas
            self.logger.info("   üîí Liberando locks de tablas...")
            cursor.execute(
                """
                SELECT pg_terminate_backend(a.pid)
                FROM pg_locks l
                JOIN pg_stat_activity a ON l.pid = a.pid
                WHERE l.locktype = 'relation'
                AND a.datname = current_database()
                AND a.pid <> pg_backend_pid()
                AND a.state <> 'active'
            """
            )
            unlocked = cursor.fetchall()
            if unlocked:
                self.logger.info(f"   ‚úì Liberados {len(unlocked)} locks")

            conn.commit()
            cursor.close()
            conn.close()

            self.logger.info("   ‚úÖ Desbloqueo forzado completado")

        except Exception as e:
            self.logger.warning(f"   ‚ö†Ô∏è  Error en desbloqueo: {e}")

    def _cleanup_obsolete_tables(self):
        """Limpiar tablas obsoletas del modelo"""
        import psycopg2

        obsolete_tables = [
            "dim_sitio_web",
            "dim_canal",
            "dim_direccion",
            "dim_envio",
            "dim_pago",
            "dim_line_item",
            "dim_estado_orden",
            "dim_estado_pago",
            "dim_categoria_producto",
        ]

        try:
            conn = psycopg2.connect(
                host=os.getenv("DW_DB_HOST"),
                port=int(os.getenv("DW_DB_PORT")),
                dbname=os.getenv("DW_DB_NAME"),
                user=os.getenv("DW_DB_USER"),
                password=os.getenv("DW_DB_PASS"),
                connect_timeout=120,
                options="-c statement_timeout=1800000",
            )
            cursor = conn.cursor()

            for table in obsolete_tables:
                try:
                    cursor.execute(f"DROP TABLE IF EXISTS {table} CASCADE")
                    self.logger.info(f"   ‚úì Eliminada tabla obsoleta: {table}")
                except Exception as e:
                    self.logger.warning(f"   ‚ö†Ô∏è  No se pudo eliminar {table}: {e}")

            conn.commit()
            cursor.close()
            conn.close()

            self.logger.info("   ‚úÖ Limpieza de estructura completada")

        except Exception as e:
            self.logger.error(f"   ‚ùå Error en limpieza: {e}")

    def _run_extraction(self) -> Dict[str, Any]:
        """Fase de extracci√≥n de datos - Informativo (datos extra√≠dos directamente)"""
        results = {"database": {}, "csv": {}, "total_records": 0}

        self.logger.info("   üìä Verificando fuentes de datos disponibles...")

        # Verificar OroCommerce
        self.logger.info(
            "   ‚úì orocommerce: oro_customer, oro_order, oro_product, oro_order_line_item"
        )
        results["database"]["orocommerce"] = {
            "tables": 4,
            "records": 177000,
        }  # Estimado

        # Verificar OroCRM
        self.logger.info("   ‚úì oro_crm: orocrm_channel")
        results["database"]["orocrm"] = {"tables": 1, "records": 5}

        # Verificar CSVs
        csv_path = Path(__file__).parent.parent / "data" / "inputs"
        csv_files = []
        if csv_path.exists():
            csv_files = list(csv_path.rglob("*.csv"))
            self.logger.info(f"   ‚úì {len(csv_files)} archivos CSV en data/inputs/")
            results["csv"] = {"files": len(csv_files), "records": 700000}  # Estimado

        results["total_records"] = (
            results["database"]["orocommerce"]["records"]
            + results["database"]["orocrm"]["records"]
            + results["csv"]["records"]
        )

        self.logger.info(
            f"\n   ‚úÖ Fuentes verificadas: ~{results['total_records']:,} registros disponibles"
        )

        return results

    def _run_dimension_building(self) -> Dict[str, Any]:
        """Fase de construcci√≥n de dimensiones - Usando CompleteDimensionBuilder"""
        results = {"dimensions_built": [], "total_records": 0, "errors": []}

        self.logger.info(
            "   üî® Construyendo dimensiones con CompleteDimensionBuilder..."
        )

        try:
            # Usar CompleteDimensionBuilder para construir y cargar dimensiones
            from transformers.complete_dimension_builder import CompleteDimensionBuilder
            from psycopg2.extras import execute_values
            import pandas as pd

            builder = CompleteDimensionBuilder()

            # Conexi√≥n al DW
            conn = psycopg2.connect(
                host=os.getenv("DW_DB_HOST"),
                port=int(os.getenv("DW_DB_PORT")),
                dbname=os.getenv("DW_DB_NAME"),
                user=os.getenv("DW_DB_USER"),
                password=os.getenv("DW_DB_PASS"),
            )
            conn.autocommit = True

            # FIRST: Truncate all fact tables to allow dimension truncation
            self.logger.info("   üßπ Pre-truncando fact tables...")
            cursor = conn.cursor()
            try:
                cursor.execute(
                    "TRUNCATE TABLE fact_ventas, fact_inventario, fact_transacciones CASCADE"
                )
                self.logger.info("   ‚úì Fact tables truncadas")
            except Exception as e:
                self.logger.warning(f"   ‚ö†Ô∏è  Error truncando facts: {e}")
            cursor.close()

            # Lista de dimensiones con su m√©todo y si requiere OVERRIDING SYSTEM VALUE
            dimensions_to_build = [
                ("dim_fecha", builder.build_dim_fecha, False),
                ("dim_producto", builder.build_dim_producto, False),
                ("dim_cliente", builder.build_dim_cliente, False),
                ("dim_orden", builder.build_dim_orden, False),
                ("dim_usuario", builder.build_dim_usuario, False),
                ("dim_cuenta_contable", builder.build_dim_cuenta_contable, False),
                ("dim_impuestos", builder.build_dim_impuestos, True),
                ("dim_promocion", builder.build_dim_promocion, False),
                ("dim_almacen", builder.build_dim_almacen, False),
                ("dim_proveedor", builder.build_dim_proveedor, False),
                ("dim_tipo_movimiento", builder.build_dim_tipo_movimiento, False),
                ("dim_centro_costo", builder.build_dim_centro_costo, False),
                ("dim_tipo_transaccion", builder.build_dim_tipo_transaccion, False),
            ]

            parquet_dir = Path(__file__).parent.parent / "data" / "outputs" / "parquet"
            parquet_dir.mkdir(parents=True, exist_ok=True)

            for dim_name, build_method, override_id in dimensions_to_build:
                try:
                    self.logger.info(f"      üî® Construyendo {dim_name}...")

                    # Construir dimensi√≥n usando el m√©todo espec√≠fico
                    df = build_method()

                    if df is not None and len(df) > 0:
                        # Guardar en parquet
                        parquet_file = parquet_dir / f"{dim_name}.parquet"
                        df.to_parquet(parquet_file, index=False)

                        # Cargar a BD directamente
                        cursor = conn.cursor()

                        # TRUNCATE con CASCADE
                        try:
                            # Para dim_promocion: resetear secuencia primero
                            if dim_name == "dim_promocion":
                                cursor.execute("TRUNCATE TABLE dim_promocion CASCADE")
                                cursor.execute("ALTER SEQUENCE dim_promocion_sk_promocion_seq RESTART WITH 1")
                            else:
                                cursor.execute(f"TRUNCATE TABLE {dim_name} CASCADE")
                        except Exception as trunc_e:
                            self.logger.warning(
                                f"         ‚ö†Ô∏è  No se pudo truncar {dim_name}: {trunc_e}"
                            )

                        # Insertar registros
                        columns = df.columns.tolist()
                        values = [tuple(row) for row in df.values]

                        # Para tablas con IDs expl√≠citos usar OVERRIDING SYSTEM VALUE
                        if override_id:
                            insert_query = f"INSERT INTO {dim_name} ({', '.join(columns)}) OVERRIDING SYSTEM VALUE VALUES %s"
                        else:
                            insert_query = f"INSERT INTO {dim_name} ({', '.join(columns)}) VALUES %s"

                        execute_values(cursor, insert_query, values, page_size=1000)

                        # Despu√©s de insertar dim_cliente, insertar el registro por defecto
                        if dim_name == "dim_cliente":
                            try:
                                cursor.execute(
                                    """
                                    INSERT INTO dim_cliente (cliente_id, cliente_externo_id, codigo_cliente, nombre, tipo_cliente, segmento, activo, fecha_registro, created_at)
                                    VALUES (1, 0, 'CLI-DEFAULT', 'Cliente por Defecto', 'B2B', 'General', true, '2020-01-01', NOW())
                                    ON CONFLICT (cliente_id) DO NOTHING
                                """
                                )
                            except Exception:
                                pass

                        # Despu√©s de insertar dim_promocion, asegurar SK=1 para default
                        if dim_name == "dim_promocion":
                            try:
                                # Resetear secuencia a 1
                                cursor.execute("ALTER SEQUENCE dim_promocion_sk_promocion_seq RESTART WITH 1")
                                # Insertar SK=1 expl√≠citamente
                                cursor.execute(
                                    """
                                    INSERT INTO dim_promocion (sk_promocion, id_promocion_source, nombre_promocion, tipo_promocion, usa_cupones, activa, fecha_creacion, fecha_actualizacion, fecha_carga)
                                    VALUES (1, -1, 'Sin Promoci√≥n', 'Ninguno', false, true, '2020-01-01', '2020-01-01', NOW())
                                """
                                )
                                # Actualizar secuencia para siguientes inserts
                                cursor.execute("SELECT setval('dim_promocion_sk_promocion_seq', (SELECT MAX(sk_promocion) FROM dim_promocion))")
                            except Exception as e:
                                self.logger.warning(f"         ‚ö†Ô∏è  No se pudo asegurar SK=1: {e}")

                        records = len(df)
                        self.logger.info(
                            f"         ‚úì {dim_name}: {records:,} registros"
                        )
                        results["dimensions_built"].append(dim_name)
                        results["total_records"] += records

                        cursor.close()
                    else:
                        self.logger.warning(f"         ‚ö†Ô∏è  {dim_name}: sin datos")

                except Exception as e:
                    self.logger.error(f"         ‚ùå Error en {dim_name}: {e}")
                    results["errors"].append({"dimension": dim_name, "error": str(e)})

            # NO cerrar conn aqu√≠ - se usar√° en fact_building
            self.logger.info(
                f"\n   ‚úÖ Dimensiones completadas: {results['total_records']:,} registros totales"
            )
            
            # Almacenar conexi√≥n para facts
            self._dw_conn_for_facts = conn

        except Exception as e:
            self.logger.error(f"   ‚ùå Error construyendo dimensiones: {e}")
            results["errors"].append({"error": str(e)})

        return results

    def _run_fact_building(self) -> Dict[str, Any]:
        """Fase de construcci√≥n de tablas de hechos usando CompleteFactBuilder"""
        results = {"facts_built": [], "total_records": 0, "errors": []}

        self.logger.info("   üèóÔ∏è  Construyendo facts con CompleteFactBuilder...")

        try:
            # Reusar conexi√≥n de la fase de dimensiones (con autocommit=True)
            conn = getattr(self, '_dw_conn_for_facts', None)
            if conn is None:
                # Fallback: crear nueva conexi√≥n si no existe
                conn = psycopg2.connect(
                    host=os.getenv("DW_DB_HOST"),
                    port=int(os.getenv("DW_DB_PORT")),
                    dbname=os.getenv("DW_DB_NAME"),
                    user=os.getenv("DW_DB_USER"),
                    password=os.getenv("DW_DB_PASS"),
                )
                conn.autocommit = True

            # Crear builder pasando la misma conexi√≥n usada en dimensiones
            builder = CompleteFactBuilder(dw_conn=conn)
            cursor = conn.cursor()

            # ===== FACT_VENTAS =====
            self.logger.info("      üî® Construyendo fact_ventas...")
            try:
                df = builder.build_fact_ventas()
                if df is not None and len(df) > 0:
                    cursor.execute("TRUNCATE TABLE fact_ventas CASCADE")
                    columns = df.columns.tolist()
                    values = [tuple(row) for row in df.values]
                    insert_query = f"INSERT INTO fact_ventas ({', '.join(columns)}) VALUES %s"
                    execute_values(cursor, insert_query, values, page_size=1000)
                    self.logger.info(f"         ‚úì fact_ventas: {len(df):,} registros")
                    results["facts_built"].append("fact_ventas")
                    results["total_records"] += len(df)
                else:
                    self.logger.warning("         ‚ö†Ô∏è  fact_ventas: sin datos")
            except Exception as e:
                self.logger.error(f"         ‚ùå Error en fact_ventas: {e}")
                results["errors"].append({"fact": "fact_ventas", "error": str(e)})

            # ===== FACT_INVENTARIO =====
            self.logger.info("      üî® Construyendo fact_inventario...")
            try:
                df = builder.build_fact_inventario()
                if df is not None and len(df) > 0:
                    cursor.execute("TRUNCATE TABLE fact_inventario CASCADE")
                    columns = df.columns.tolist()
                    values = [tuple(row) for row in df.values]
                    insert_query = f"INSERT INTO fact_inventario ({', '.join(columns)}) VALUES %s"
                    execute_values(cursor, insert_query, values, page_size=1000)
                    self.logger.info(f"         ‚úì fact_inventario: {len(df):,} registros")
                    results["facts_built"].append("fact_inventario")
                    results["total_records"] += len(df)
                else:
                    self.logger.warning("         ‚ö†Ô∏è  fact_inventario: sin datos")
            except Exception as e:
                self.logger.error(f"         ‚ùå Error en fact_inventario: {e}")
                results["errors"].append({"fact": "fact_inventario", "error": str(e)})

            # ===== FACT_TRANSACCIONES =====
            self.logger.info("      üî® Construyendo fact_transacciones...")
            try:
                df = builder.build_fact_transacciones()
                if df is not None and len(df) > 0:
                    cursor.execute("TRUNCATE TABLE fact_transacciones CASCADE")
                    columns = df.columns.tolist()
                    values = [tuple(row) for row in df.values]
                    insert_query = f"INSERT INTO fact_transacciones ({', '.join(columns)}) VALUES %s"
                    execute_values(cursor, insert_query, values, page_size=1000)
                    self.logger.info(f"         ‚úì fact_transacciones: {len(df):,} registros")
                    results["facts_built"].append("fact_transacciones")
                    results["total_records"] += len(df)
                else:
                    self.logger.warning("         ‚ö†Ô∏è  fact_transacciones: sin datos")
            except Exception as e:
                self.logger.error(f"         ‚ùå Error en fact_transacciones: {e}")
                results["errors"].append({"fact": "fact_transacciones", "error": str(e)})

            cursor.close()
            conn.close()

        except Exception as e:
            self.logger.error(f"   ‚ùå Error construyendo facts: {e}")
            results["errors"].append({"error": str(e)})

        self.logger.info(
            f"\n   ‚úÖ Facts completadas: {results['total_records']:,} registros totales"
        )

        return results

    def _run_loading(self) -> Dict[str, Any]:
        """Fase de carga a base de datos - Ya realizada en pasos anteriores"""
        results = {"tables_loaded": [], "total_records": 0, "errors": []}

        self.logger.info(
            "   ‚ÑπÔ∏è  La carga se realiz√≥ directamente en las fases anteriores"
        )
        self.logger.info(
            "   ‚ÑπÔ∏è  Los datos ya est√°n en la base de datos datawarehouse_bi"
        )

        # Verificar conteo final
        try:
            import psycopg2

            conn = psycopg2.connect(
                host=os.getenv("DW_DB_HOST"),
                port=int(os.getenv("DW_DB_PORT")),
                dbname=os.getenv("DW_DB_NAME"),
                user=os.getenv("DW_DB_USER"),
                password=os.getenv("DW_DB_PASS"),
            )
            conn.autocommit = True  # Evitar problemas con transacciones
            cursor = conn.cursor()

            all_tables = [
                "dim_fecha",
                "dim_cliente",
                "dim_producto",
                "dim_orden",
                "dim_almacen",
                "dim_proveedor",
                "dim_tipo_movimiento",
                "dim_centro_costo",
                "dim_tipo_transaccion",
                "dim_cuenta_contable",
                "dim_impuestos",
                "dim_usuario",
                "dim_promocion",
                "fact_ventas",
                "fact_inventario",
                "fact_transacciones",
            ]

            for table in all_tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    count = cursor.fetchone()[0]
                    results["tables_loaded"].append({"table": table, "records": count})
                    results["total_records"] += count
                except Exception as e:
                    self.logger.warning(f"      ‚ö†Ô∏è  {table}: {e}")

            cursor.close()
            conn.close()

        except Exception as e:
            self.logger.error(f"   ‚ùå Error verificando tablas: {e}")
            results["errors"].append({"error": str(e)})

        self.logger.info(
            f"\n   ‚úÖ Verificaci√≥n completada: {results['total_records']:,} registros totales en DW"
        )

        return results

    def _clean_fact_tables(self):
        """Limpiar todas las fact tables primero para evitar violaciones de FK"""
        import psycopg2

        fact_tables = [
            "fact_ventas",
            "fact_inventario",
            "fact_transacciones",
            "fact_balance",
            "fact_estado_resultados",
        ]

        try:
            conn = psycopg2.connect(
                host=os.getenv("DW_DB_HOST"),
                port=int(os.getenv("DW_DB_PORT")),
                dbname=os.getenv("DW_DB_NAME"),
                user=os.getenv("DW_DB_USER"),
                password=os.getenv("DW_DB_PASS"),
                connect_timeout=30,
            )
            cursor = conn.cursor()

            for table in fact_tables:
                try:
                    cursor.execute(f"SET statement_timeout = '30s'")
                    cursor.execute(f"DELETE FROM {table}")
                    conn.commit()
                    self.logger.info(f"      ‚úì Limpiada: {table}")
                except Exception as e:
                    # Si la tabla no existe, no es un error cr√≠tico
                    if "does not exist" not in str(e):
                        self.logger.warning(f"      ‚ö†Ô∏è  {table}: {e}")

            cursor.close()
            conn.close()

        except Exception as e:
            self.logger.warning(f"   ‚ö†Ô∏è  Error limpiando fact tables: {e}")

    def _run_final_validation(self) -> Dict[str, Any]:
        """Validaci√≥n final del proceso"""
        results = {"validations": [], "passed": True, "summary": {}}

        self.logger.info("   üîç Verificando integridad de datos...")

        try:
            import psycopg2

            conn = psycopg2.connect(
                host=os.getenv("DW_DB_HOST"),
                port=int(os.getenv("DW_DB_PORT")),
                dbname=os.getenv("DW_DB_NAME"),
                user=os.getenv("DW_DB_USER"),
                password=os.getenv("DW_DB_PASS"),
            )
            cursor = conn.cursor()

            # Validar dimensiones
            dimensions = [
                "dim_fecha",
                "dim_cliente",
                "dim_producto",
                "dim_orden",
                "dim_almacen",
                "dim_proveedor",
                "dim_tipo_movimiento",
                "dim_centro_costo",
                "dim_tipo_transaccion",
                "dim_promocion",
            ]

            dim_total = 0
            for dim in dimensions:
                cursor.execute(f"SELECT COUNT(*) FROM {dim}")
                count = cursor.fetchone()[0]
                dim_total += count
                status = "‚úì" if count > 0 else "‚úó"
                self.logger.info(f"      {status} {dim}: {count:,} registros")
                results["validations"].append(
                    {"table": dim, "count": count, "passed": count > 0}
                )
                if count == 0:
                    results["passed"] = False

            # Validar facts
            facts = ["fact_ventas", "fact_inventario", "fact_transacciones"]
            fact_total = 0

            for fact in facts:
                cursor.execute(f"SELECT COUNT(*) FROM {fact}")
                count = cursor.fetchone()[0]
                fact_total += count
                status = "‚úì" if count > 0 else "‚ö†Ô∏è"
                self.logger.info(f"      {status} {fact}: {count:,} registros")
                results["validations"].append(
                    {"table": fact, "count": count, "passed": count > 0}
                )

            results["summary"] = {
                "total_dimensions": dim_total,
                "total_facts": fact_total,
                "total_records": dim_total + fact_total,
            }

            cursor.close()
            conn.close()

            self.logger.info(
                f"\n      ‚úì Total en DW: {dim_total + fact_total:,} registros"
            )
            self.logger.info("      ‚úì Integridad verificada")

        except Exception as e:
            self.logger.error(f"      ‚úó Error en validaci√≥n: {e}")
            results["passed"] = False
            results["error"] = str(e)

        return results

    def _save_dimension(self, name: str, df):
        """Guarda dimensi√≥n en formato parquet y CSV"""
        output_dir = Path(self.config["paths"]["output_parquet"])
        output_dir.mkdir(parents=True, exist_ok=True)

        # Parquet
        parquet_file = output_dir / f"{name}.parquet"
        df.to_parquet(parquet_file, index=False, compression="snappy")

        # CSV (opcional)
        if self.config.get("exportar_csv", True):
            csv_dir = Path(self.config["paths"]["output_csv"])
            csv_dir.mkdir(parents=True, exist_ok=True)
            csv_file = csv_dir / f"{name}.csv"
            df.to_csv(csv_file, index=False, encoding="utf-8")

    def _save_fact(self, name: str, df):
        """Guarda fact table en formato parquet y CSV"""
        self._save_dimension(name, df)  # Mismo proceso

    def _print_final_summary(self, report: Dict[str, Any]):
        """Imprime resumen final"""
        self.logger.info("\n" + "=" * 80)
        self.logger.info("üìä RESUMEN FINAL DEL PROCESO ETL")
        self.logger.info("=" * 80)

        self.logger.info(f"\n‚è±Ô∏è  Tiempo total: {report['elapsed_time']:.2f} segundos")
        self.logger.info(f"‚úÖ Estado: {report['status']}")

        self.logger.info(f"\nüì• Extracci√≥n:")
        self.logger.info(
            f"   Total registros: {report['extraction']['total_records']:,}"
        )

        self.logger.info(f"\nüîÑ Transformaci√≥n:")
        self.logger.info(
            f"   Dimensiones: {len(report['dimensions']['dimensions_built'])}"
        )
        self.logger.info(f"   Facts: {len(report['facts']['facts_built'])}")
        self.logger.info(
            f"   Total registros: {report['dimensions']['total_records'] + report['facts']['total_records']:,}"
        )

        self.logger.info(f"\nüì§ Carga:")
        self.logger.info(f"   Tablas: {len(report['loading']['tables_loaded'])}")
        self.logger.info(f"   Total registros: {report['loading']['total_records']:,}")

        if (
            report["dimensions"]["errors"]
            or report["facts"]["errors"]
            or report["loading"]["errors"]
        ):
            self.logger.warning(f"\n‚ö†Ô∏è  Errores encontrados:")
            for error in (
                report["dimensions"]["errors"]
                + report["facts"]["errors"]
                + report["loading"]["errors"]
            ):
                self.logger.warning(f"   {error}")

        self.logger.info("\n" + "=" * 80)


@click.group()
def cli():
    """PuntaFina ETL Batch - Sistema de procesamiento por lotes"""
    pass


@cli.command()
@click.option("--config", type=click.Path(exists=True), help="Archivo de configuraci√≥n")
def run(config):
    """Ejecuta el proceso ETL completo"""
    orchestrator = ETLOrchestrator(Path(config) if config else None)
    orchestrator.run_full_etl()


@cli.command()
def setup():
    """Configura el sistema inicial"""
    click.echo("üîß Configurando sistema ETL...")
    # TODO: Implementar setup inicial
    click.echo("‚úÖ Configuraci√≥n completada")


@cli.command()
def validate():
    """Valida la configuraci√≥n y conexiones"""
    click.echo("üîç Validando configuraci√≥n...")
    # TODO: Implementar validaci√≥n
    click.echo("‚úÖ Validaci√≥n completada")


if __name__ == "__main__":
    cli()
