# ğŸ“˜ GUÃA DE USO - PUNTAFINA ETL BATCH
## Sistema de Procesamiento por Lotes para Data Warehouse

---

## ğŸ“‹ Tabla de Contenidos

- [IntroducciÃ³n](#introducciÃ³n)
- [Conceptos BÃ¡sicos](#conceptos-bÃ¡sicos)
- [EjecuciÃ³n del ETL](#ejecuciÃ³n-del-etl)
- [Procesamiento por Lotes](#procesamiento-por-lotes)
- [ValidaciÃ³n y PoblaciÃ³n de Datos](#validaciÃ³n-y-poblaciÃ³n-de-datos)
- [Monitoreo y Logs](#monitoreo-y-logs)
- [RecuperaciÃ³n de Errores](#recuperaciÃ³n-de-errores)
- [Casos de Uso Comunes](#casos-de-uso-comunes)

---

## ğŸ¯ IntroducciÃ³n

El sistema ETL Batch de PuntaFina estÃ¡ diseÃ±ado para:

- âœ… Procesar grandes volÃºmenes de datos eficientemente
- âœ… Mantener coherencia entre OroCommerce, OroCRM y archivos CSV
- âœ… Validar y poblar datos faltantes automÃ¡ticamente
- âœ… Recuperarse de errores con checkpoints
- âœ… Ejecutar en Ubuntu 22.04 de manera optimizada

---

## ğŸ§  Conceptos BÃ¡sicos

### Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FUENTES DE DATOS                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ OroCommerce  â”‚  â”‚   OroCRM     â”‚  â”‚  CSV Files   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚               â”‚                â”‚
              v               v                v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EXTRACCIÃ“N (Extractors)                    â”‚
â”‚  - DatabaseExtractor: Extrae de bases de datos          â”‚
â”‚  - CSVExtractor: Extrae de archivos CSV                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            TRANSFORMACIÃ“N (Transformers)                â”‚
â”‚  - DimensionBuilder: Construye dimensiones              â”‚
â”‚  - FactBuilder: Construye tablas de hechos              â”‚
â”‚  - DataValidator: Valida y puebla datos                 â”‚
â”‚  - BatchProcessor: Procesa por lotes                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CARGA (Loaders)                            â”‚
â”‚  - DatabaseLoader: Carga a Data Warehouse               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DATA WAREHOUSE                          â”‚
â”‚  - Dimensiones (20 tablas)                              â”‚
â”‚  - Hechos (5 tablas)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo del Proceso ETL

1. **ExtracciÃ³n**: Lee datos de bases de datos y CSVs
2. **ValidaciÃ³n**: Verifica coherencia y calidad de datos
3. **PoblaciÃ³n**: Completa datos faltantes automÃ¡ticamente
4. **TransformaciÃ³n**: Construye dimensiones y facts
5. **Carga**: Inserta en Data Warehouse
6. **VerificaciÃ³n**: Valida integridad final

---

## ğŸš€ EjecuciÃ³n del ETL

### Comando BÃ¡sico

```bash
# Activar entorno virtual
source venv/bin/activate

# Ejecutar ETL completo
cd etl_batch
python main.py run
```

### Comandos Disponibles

```bash
# Ver ayuda
python main.py --help

# Ejecutar ETL completo
python main.py run

# Validar configuraciÃ³n
python main.py validate

# Setup inicial
python main.py setup

# Con configuraciÃ³n personalizada
python main.py run --config /path/to/config.yaml
```

### Salida TÃ­pica

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸª PUNTAFINA ETL BATCH - PROCESO COMPLETO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¥ FASE 1: EXTRACCIÃ“N
   ğŸ“Š Extrayendo de OroCommerce...
      âœ“ 25,342 registros
   ğŸ“Š Extrayendo de OroCRM...
      âœ“ 1,234 registros
   ğŸ“ Extrayendo de archivos CSV...
      âœ“ 2,142 registros
   
   âœ… ExtracciÃ³n completada: 28,718 registros totales

ğŸ”„ FASE 2: TRANSFORMACIÃ“N - DIMENSIONES
   ğŸ”¨ Construyendo dim_fecha...
      âœ“ 2,557 registros
   ğŸ”¨ Construyendo dim_cliente...
      âœ“ 856 registros
   ...

ğŸ”„ FASE 3: TRANSFORMACIÃ“N - TABLAS DE HECHOS
   ğŸ—ï¸  Construyendo fact_ventas...
      âœ“ 30,245 registros
   ...

ğŸ“¤ FASE 4: CARGA
   ğŸš› Cargando dimensiones...
      ğŸ“¤ dim_fecha...
         âœ“ 2,557 registros
   ...

âœ… FASE 5: VALIDACIÃ“N FINAL
   ğŸ” Verificando integridad de datos...
      âœ“ Integridad verificada

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š RESUMEN FINAL DEL PROCESO ETL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â±ï¸  Tiempo total: 125.34 segundos
âœ… Estado: success

ğŸ“¥ ExtracciÃ³n:
   Total registros: 28,718

ğŸ”„ TransformaciÃ³n:
   Dimensiones: 20
   Facts: 5
   Total registros: 145,623

ğŸ“¤ Carga:
   Tablas: 25
   Total registros: 145,623
```

---

## ğŸ”„ Procesamiento por Lotes

### ConfiguraciÃ³n de Lotes

En `etl_config.yaml`:

```yaml
batch:
  chunk_size: 1000        # Registros por lote
  max_workers: 4          # Procesos paralelos
  timeout: 300            # Timeout por lote (segundos)
  max_retries: 3          # Reintentos en caso de error
  retry_delay: 5          # Espera entre reintentos (segundos)
  max_memory_mb: 512      # Memoria mÃ¡xima por worker
```

### CÃ³mo Funciona

1. **DivisiÃ³n**: Datos se dividen en chunks de `chunk_size` registros
2. **Procesamiento Paralelo**: Hasta `max_workers` procesando simultÃ¡neamente
3. **Checkpoints**: Se guarda progreso cada `checkpoint_interval` lotes
4. **RecuperaciÃ³n**: Si falla, reanuda desde Ãºltimo checkpoint

### Ejemplo de Uso

```python
from core.batch_processor import BatchProcessor, BatchConfig

# Configurar
config = BatchConfig(
    chunk_size=500,
    max_workers=2,
    enable_checkpoints=True
)

processor = BatchProcessor(config)

# Procesar DataFrame en lotes
def process_chunk(df):
    # Tu lÃ³gica de transformaciÃ³n
    return df.apply(lambda x: x * 2)

results = processor.process_dataframe(
    df=my_large_dataframe,
    process_func=process_chunk,
    job_name="mi_proceso"
)
```

### Procesamiento Streaming

Para archivos MUY grandes que no caben en memoria:

```python
from core.batch_processor import StreamingBatchProcessor

streaming = StreamingBatchProcessor(config)

results = streaming.process_large_file(
    file_path="data/huge_file.csv",
    process_func=process_chunk,
    job_name="streaming_job",
    file_format="csv"
)
```

---

## âœ… ValidaciÃ³n y PoblaciÃ³n de Datos

### Validaciones AutomÃ¡ticas

El sistema valida automÃ¡ticamente:

1. **Estructura**: Columnas requeridas existen
2. **Tipos de Datos**: Tipos correctos (int, float, date, etc.)
3. **Valores Obligatorios**: Campos required no son nulos
4. **Integridad Referencial**: Foreign keys vÃ¡lidas
5. **Rangos**: Valores dentro de rangos permitidos
6. **Duplicados**: Elimina duplicados por primary key

### PoblaciÃ³n AutomÃ¡tica

Si faltan datos, el sistema los puebla automÃ¡ticamente:

```yaml
population_rules:
  # Generar IDs automÃ¡ticamente
  auto_generate_ids: true
  id_prefix: "AUTO_"
  
  # Valores por defecto
  default_values:
    estado: "activo"
    tipo: "general"
    moneda: "USD"
    pais: "El Salvador"
    
  # Fechas por defecto
  default_dates:
    created_at: "current_timestamp"
    updated_at: "current_timestamp"
```

### Mantener SimetrÃ­a

El validador asegura simetrÃ­a entre fuentes:

```python
from core.data_validator import DataValidator

validator = DataValidator(config)

# Validar simetrÃ­a
symmetry_report = validator.validate_symmetry(
    db_data=oro_commerce_data,
    csv_data=csv_data,
    key_columns=['id', 'codigo']
)

# Fusionar y reconciliar
merged = validator.merge_and_reconcile(
    db_data=oro_commerce_data,
    csv_data=csv_data,
    key_columns=['id'],
    priority='db'  # BD tiene prioridad
)
```

### Reportes de ValidaciÃ³n

Cada validaciÃ³n genera un reporte:

```json
{
  "source": "dim_producto",
  "original_rows": 1000,
  "final_rows": 1050,
  "rows_added": 50,
  "validations": [
    {
      "validation": "structure",
      "status": "passed",
      "missing_columns": []
    },
    {
      "validation": "required_fields",
      "status": "fixed",
      "issues": ["precio: 50 valores poblados"]
    }
  ],
  "populations": [
    {
      "population": "missing_data",
      "status": "completed",
      "fields_populated": ["estado: 25 valores", "moneda: 25 valores"]
    }
  ]
}
```

---

## ğŸ“Š Monitoreo y Logs

### Niveles de Log

- **DEBUG**: InformaciÃ³n detallada para debugging
- **INFO**: InformaciÃ³n general del proceso (default)
- **WARNING**: Advertencias no crÃ­ticas
- **ERROR**: Errores que requieren atenciÃ³n
- **CRITICAL**: Errores crÃ­ticos que detienen el proceso

### UbicaciÃ³n de Logs

```
logs/
â”œâ”€â”€ etl/                        # Logs del proceso ETL
â”‚   â””â”€â”€ ETLOrchestrator_20260101.log
â”œâ”€â”€ audit/                      # AuditorÃ­a de cambios
â”‚   â””â”€â”€ audit_20260101.log
â””â”€â”€ errors/                     # Solo errores
    â””â”€â”€ errors_20260101.log
```

### Ver Logs en Tiempo Real

```bash
# Ver log principal
tail -f logs/etl/ETLOrchestrator_*.log

# Ver solo errores
tail -f logs/errors/errors_*.log

# Buscar texto especÃ­fico
grep -r "dim_fecha" logs/

# Ãšltimas 100 lÃ­neas
tail -n 100 logs/etl/ETLOrchestrator_*.log
```

### Logs en Formato JSON

Los logs pueden configurarse en formato JSON para anÃ¡lisis:

```yaml
monitoring:
  log_format: "json"
```

Ejemplo de log JSON:

```json
{
  "timestamp": "2026-01-01T10:30:45.123456",
  "level": "INFO",
  "logger": "ETLOrchestrator",
  "message": "ExtracciÃ³n completada: 28,718 registros",
  "module": "main",
  "function": "_run_extraction",
  "line": 156
}
```

### MÃ©tricas del Proceso

El sistema recolecta mÃ©tricas automÃ¡ticamente:

```json
{
  "duration_seconds": 125.34,
  "records_processed": 145623,
  "records_failed": 12,
  "success_rate": 99.99,
  "tables_processed": 25,
  "errors_count": 1,
  "warnings_count": 5,
  "memory_usage_mb": 512.45,
  "cpu_percent": 45.2
}
```

---

## ğŸ”§ RecuperaciÃ³n de Errores

### Checkpoints AutomÃ¡ticos

El sistema guarda checkpoints automÃ¡ticamente:

```
data/checkpoints/
â””â”€â”€ mi_proceso.checkpoint
```

Contenido del checkpoint:

```json
{
  "job_name": "dimension_builder",
  "chunk_id": 150,
  "timestamp": "2026-01-01T10:35:22",
  "total_processed": 150000,
  "total_failed": 25
}
```

### Reanudar desde Checkpoint

Si el proceso falla, automÃ¡ticamente reanuda:

```
ğŸ“ Reanudando desde lote 150
```

### Limpiar Checkpoints

Para forzar re-ejecuciÃ³n completa:

```bash
# Eliminar todos los checkpoints
rm -rf data/checkpoints/*.checkpoint

# Eliminar checkpoint especÃ­fico
rm data/checkpoints/mi_proceso.checkpoint
```

### Manejo de Errores

Estrategias de manejo:

1. **Reintentos**: Hasta 3 intentos por lote
2. **Skip**: ContinÃºa con siguiente lote
3. **Fail**: Detiene proceso (crÃ­tico)

ConfiguraciÃ³n:

```yaml
batch:
  max_retries: 3
  retry_delay: 5

recovery:
  enable_checkpoints: true
  resume_on_failure: true
```

---

## ğŸ“š Casos de Uso Comunes

### Caso 1: Carga Inicial Completa

```bash
# Primera vez - Cargar todo desde cero
python main.py run
```

### Caso 2: Carga Incremental Diaria

```yaml
# Configurar en etl_config.yaml
loading:
  strategy: "incremental"

# Ejecutar
python main.py run
```

### Caso 3: Actualizar Solo Dimensiones

```python
from transformers.dimension_builder import DimensionBuilder

builder = DimensionBuilder(config)

# Construir solo dim_fecha
dim_fecha = builder.build('dim_fecha')
```

### Caso 4: Validar Datos sin Cargar

```bash
# Solo validaciÃ³n
python main.py validate
```

### Caso 5: Poblar CSV Faltante

```python
from core.data_validator import DataValidator
from extractors.csv_extractor import CSVExtractor

validator = DataValidator(config)
csv_extractor = CSVExtractor(config)

# Cargar CSV
df = csv_extractor.extract_file('ventas', 'estados_orden.csv')

# Validar y poblar
df_validated, report = validator.validate_and_populate(
    df,
    schema={'columns': {...}},
    source_name='estados_orden'
)

# Guardar CSV actualizado
csv_extractor.save_file(df_validated, 'ventas', 'estados_orden_completo.csv')
```

### Caso 6: Procesar Archivo Grande

```python
from core.batch_processor import StreamingBatchProcessor, BatchConfig

config = BatchConfig(chunk_size=1000)
processor = StreamingBatchProcessor(config)

def process_chunk(df):
    # TransformaciÃ³n
    df['total'] = df['cantidad'] * df['precio']
    return df

processor.process_large_file(
    'data/inputs/huge_transactions.csv',
    process_chunk,
    'huge_file_job'
)
```

---

## ğŸ¯ Mejores PrÃ¡cticas

### 1. TamaÃ±o de Lote Ã“ptimo

- **RAM < 4GB**: chunk_size = 500, max_workers = 2
- **RAM 4-8GB**: chunk_size = 1000, max_workers = 4
- **RAM > 8GB**: chunk_size = 2000, max_workers = 8

### 2. Monitoreo Regular

```bash
# Script de monitoreo
watch -n 5 'tail -n 20 logs/etl/ETLOrchestrator_*.log'
```

### 3. Backups Antes de Carga

```bash
# Backup automÃ¡tico antes de carga
pg_dump DW_oro > backup_$(date +%Y%m%d).sql
```

### 4. ValidaciÃ³n Post-Carga

```sql
-- Verificar conteos
SELECT 'dim_fecha' as tabla, COUNT(*) FROM dim_fecha
UNION ALL
SELECT 'dim_cliente', COUNT(*) FROM dim_cliente
UNION ALL
SELECT 'fact_ventas', COUNT(*) FROM fact_ventas;
```

### 5. Limpieza PeriÃ³dica

```bash
# Limpiar logs antiguos (mayores a 30 dÃ­as)
find logs/ -name "*.log" -mtime +30 -delete

# Limpiar checkpoints antiguos
find data/checkpoints/ -name "*.checkpoint" -mtime +7 -delete
```

---

## ğŸ†˜ Contacto y Soporte

Para mÃ¡s ayuda:
- ğŸ“– DocumentaciÃ³n: `docs/`
- ğŸ› Troubleshooting: `docs/TROUBLESHOOTING.md`
- âš™ï¸ ConfiguraciÃ³n Avanzada: `docs/ADVANCED_CONFIGURATION.md`

---

**Â¡Sistema ETL listo para producciÃ³n!** ğŸ‰
